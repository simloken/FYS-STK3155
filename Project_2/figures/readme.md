## These are all the figures

>contourWiki.png

A contour plot retrieved from the Gradient Decent Wikipedia article, for illustration purposes

>FFNNRegressionR2.png

The R2 score of a FFNN regression case for a variable learning rate using the Adam activation function

>FFNNRegressionSigmoid.png

The R2 score of a FFNN regression case for variable learning rate using the Sigmoid activation function

>intro1.png

A graph for illustration purposes

>LRBatchsize.png

Logical Regression case for variable batch size

>LREpochs.png

Logical Regression case for variable Epoch count

>LRIterations.png

Logical Regression case for variable iteration count

>LRLearning.png

Logical Regression case for variable learning rate

>MNISTManual50Neuron.png

FFNN classifcation case for 50 neurons and variable lambda and learning rate

>MNISTManual100Neuron.png

FFNN classifcation case for 100 neurons and variable lambda and learning rate

>MNISTTensor50Neuron.png

Tensorflow classifcation case for 50 neurons and variable lambda and learning rate

>MNISTTensor100Neuron.png

Tensorflow classifcation case for 100 neurons and variable lambda and learning rate

>SGDSoloOpt.png

MSE of our Stochastic Gradient Descent model without anything else for varying polynomial order

>SGDBatch.png

MSE of our Stochastic Gradient Descent model for different batch sizes and polynomial orders

>SGDEpoch.png

MSE of our Stochastic Gradient Descent model for different epoch counts and polynomial orders

>SDGIterations.png

MSE of our Stochastic Gradient Descent model for different iteration counts and polynomial orders

>SDGLearn.png

MSE of our Stochastic Gradient Descent model for different learing rates and polynomial orders

>SDGvsOLSvsRidge.png

Comparison between our previous methods from Project 1, OLS and Ridge, and our new method SGD

>theory1.png

Illustraion image for neural networks

>theory2.png

Illustration image for neural networks